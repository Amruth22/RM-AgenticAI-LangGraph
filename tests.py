"""Integration Tests for RM-AgenticAI-LangGraph

Comprehensive test suite covering:
- Configuration and environment setup
- Model functionality and performance
- System integration and workflows
- Async operations with pytest-asyncio

Tests: 17 total (no skips)
"""

import asyncio
import sys
import pytest
from datetime import datetime
from typing import Dict, Any
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

import os
import sys
from pathlib import Path

def test_environment_setup():
    """Test if environment variables are properly set."""
    print("üîç Testing Environment Setup")
    print("-" * 40)
    
    # Check for .env file
    env_file = Path(".env")
    if env_file.exists():
        print("‚úÖ .env file found")
    else:
        print("‚ö†Ô∏è .env file not found - creating from example")
        example_file = Path(".env.example")
        if example_file.exists():
            import shutil
            shutil.copy(".env.example", ".env")
            print("‚úÖ Created .env from .env.example")
        else:
            print("‚ùå .env.example not found")
            return False
    
    # Check critical environment variables
    gemini_key = os.getenv("GEMINI_API_KEY_1")
    if gemini_key and len(gemini_key) > 10:
        print(f"‚úÖ GEMINI_API_KEY_1 is set ({gemini_key[:10]}...)")
    else:
        print("‚ö†Ô∏è GEMINI_API_KEY_1 not set or invalid")
        print("   Please set your Gemini API key in the .env file")
    
    return True

def test_imports():
    """Test critical imports."""
    print("\nüß™ Testing Critical Imports")
    print("-" * 40)
    
    imports_to_test = [
        ("os", "Built-in OS module"),
        ("sys", "Built-in sys module"),
        ("pathlib", "Built-in pathlib module"),
        ("pydantic", "Pydantic for data validation"),
        ("streamlit", "Streamlit web framework"),
        ("pandas", "Pandas for data manipulation"),
        ("numpy", "NumPy for numerical computing"),
    ]
    
    failed_imports = []
    
    for module_name, description in imports_to_test:
        try:
            __import__(module_name)
            print(f"‚úÖ {module_name}: {description}")
        except ImportError as e:
            print(f"‚ùå {module_name}: {e}")
            failed_imports.append(module_name)
    
    if failed_imports:
        print(f"\n‚ö†Ô∏è {len(failed_imports)} imports failed")
        print("Run: pip install -r requirements.txt")
        return False
    else:
        print(f"\n‚úÖ All {len(imports_to_test)} imports successful")
        return True

def test_missing_imports():
    """Test for missing import issues."""
    print("\nüîç Testing Import Issues")
    print("-" * 40)
    
    try:
        # Test the specific import that was failing
        sys.path.insert(0, '.')
        from graph import ProspectAnalysisWorkflow
        print("‚úÖ ProspectAnalysisWorkflow imported successfully")
        
        # Test other critical imports
        from langraph_agents.agents.risk_assessment_agent import RiskAssessmentAgent
        from langraph_agents.agents.product_specialist_agent import ProductSpecialistAgent
        print("‚úÖ Agent imports successful")
        
        return True
        
    except ModuleNotFoundError as e:
        print(f"‚ùå Missing module: {e}")
        if "product_recommendation_workflow" in str(e):
            print("   This is a known issue - workflow files are missing")
            print("   Solution: Run python quick_fix.py")
        return False
    except Exception as e:
        print(f"‚ùå Import error: {e}")
        return False

def test_pydantic_settings():
    """Test Pydantic settings configuration."""
    print("\n‚öôÔ∏è Testing Pydantic Settings")
    print("-" * 40)
    
    try:
        # Try to import pydantic_settings
        try:
            from pydantic_settings import BaseSettings
            print("‚úÖ pydantic_settings imported successfully")
        except ImportError:
            print("‚ö†Ô∏è pydantic_settings not found, trying pydantic BaseSettings")
            from pydantic import BaseSettings
        
        # Try to load our settings
        sys.path.insert(0, '.')
        from config.settings import Settings, get_settings
        
        # Test settings instantiation
        settings = get_settings()
        print("‚úÖ Settings loaded successfully")
        print(f"   - Gemini API Key: {'Set' if settings.gemini_api_key else 'Not set'}")
        print(f"   - Log Level: {settings.log_level}")
        print(f"   - Debug Mode: {settings.debug_mode}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Settings loading failed: {e}")
        print("   This is the error you were experiencing")
        
        # Try to provide specific guidance
        if "extra_forbidden" in str(e):
            print("\nüîß SOLUTION:")
            print("   The Pydantic model needs to allow extra fields.")
            print("   Run: python quick_fix.py")
            print("   Or manually add 'extra = \"ignore\"' to the Config class")
        
        return False

def test_streamlit_compatibility():
    """Test Streamlit compatibility."""
    print("\nüåê Testing Streamlit Compatibility")
    print("-" * 40)
    
    try:
        import streamlit as st
        print("‚úÖ Streamlit imported successfully")
        
        # Test if we can import our main modules
        sys.path.insert(0, '.')
        from config.settings import get_settings
        from config.logging_config import setup_logging, get_logger
        
        print("‚úÖ Main application modules imported successfully")
        return True
        
    except Exception as e:
        print(f"‚ùå Streamlit compatibility test failed: {e}")
        return False

def main():
    """Run all tests."""
    print("üß™ Configuration and Import Test Suite")
    print("=" * 50)
    
    tests = [
        ("Environment Setup", test_environment_setup),
        ("Critical Imports", test_imports),
        ("Missing Import Issues", test_missing_imports),
        ("Pydantic Settings", test_pydantic_settings),
        ("Streamlit Compatibility", test_streamlit_compatibility),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå {test_name} test crashed: {e}")
            results.append((test_name, False))
    
    # Summary
    print("\n" + "=" * 50)
    print("üìä TEST RESULTS SUMMARY")
    print("=" * 50)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status}: {test_name}")
    
    print(f"\nOverall: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nüéâ All tests passed! Your setup is ready.")
        print("You can now run: streamlit run main.py")
    else:
        print(f"\n‚ö†Ô∏è {total - passed} tests failed. Please fix the issues above.")
        print("\nCommon fixes:")
        print("1. Run: python quick_fix.py")
        print("2. Run: pip install -r requirements.txt")
        print("3. Set GEMINI_API_KEY_1 in your .env file")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

import os
import sys
import joblib
import pandas as pd
import numpy as np
from pathlib import Path

def test_model_files():
    """Test if all required model files exist and are loadable."""
    print("üîç Testing Model Files")
    print("-" * 40)
    
    required_files = {
        "risk_profile_model.pkl": "Risk assessment model",
        "label_encoders.pkl": "Risk model label encoders", 
        "goal_success_model.pkl": "Goal success prediction model",
        "goal_success_label_encoders.pkl": "Goal model label encoders"
    }
    
    models_dir = Path("models")
    results = {}
    
    for filename, description in required_files.items():
        filepath = models_dir / filename
        
        if not filepath.exists():
            print(f"‚ùå {filename}: File not found")
            results[filename] = False
            continue
        
        try:
            # Try to load the model/encoder
            model_data = joblib.load(filepath)
            print(f"‚úÖ {filename}: {description} loaded successfully")
            
            # Basic validation
            if hasattr(model_data, 'predict'):
                print(f"   - Model type: {type(model_data).__name__}")
                if hasattr(model_data, 'classes_'):
                    print(f"   - Classes: {model_data.classes_}")
            elif isinstance(model_data, dict):
                print(f"   - Encoders count: {len(model_data)}")
                print(f"   - Encoder keys: {list(model_data.keys())}")
            
            results[filename] = True
            
        except Exception as e:
            print(f"‚ùå {filename}: Failed to load - {str(e)}")
            results[filename] = False
    
    success_count = sum(results.values())
    total_count = len(results)
    
    print(f"\nüìä Model Files: {success_count}/{total_count} loaded successfully")
    return success_count == total_count

def test_risk_model():
    """Test the risk assessment model with sample data."""
    print("\nüéØ Testing Risk Assessment Model")
    print("-" * 40)
    
    try:
        # Load models
        risk_model = joblib.load("models/risk_profile_model.pkl")
        risk_encoders = joblib.load("models/label_encoders.pkl")
        
        print("‚úÖ Risk models loaded")
        
        # Create sample data
        sample_data = {
            "age": 35,
            "annual_income": 800000,
            "current_savings": 500000,
            "target_goal_amount": 2000000,
            "investment_horizon_years": 10,
            "number_of_dependents": 2,
            "investment_experience_level": "Intermediate"
        }
        
        print(f"üìù Testing with sample prospect: {sample_data['age']} years old, ‚Çπ{sample_data['annual_income']:,} income")
        
        # Prepare data
        input_df = pd.DataFrame([sample_data])
        
        # Encode categorical variables
        for col, encoder in risk_encoders.items():
            if col in input_df.columns:
                try:
                    original_value = input_df[col].iloc[0]
                    input_df[col] = encoder.transform(input_df[col])
                    print(f"   - Encoded {col}: '{original_value}' ‚Üí {input_df[col].iloc[0]}")
                except ValueError as e:
                    print(f"   ‚ö†Ô∏è Encoding issue for {col}: {e}")
                    # Use first class as fallback
                    input_df[col] = encoder.transform([encoder.classes_[0]])[0]
        
        # Make prediction
        prediction = risk_model.predict(input_df)[0]
        probabilities = risk_model.predict_proba(input_df)[0]
        
        # Map prediction
        risk_mapping = {0: "Low", 1: "Moderate", 2: "High"}
        risk_level = risk_mapping.get(prediction, f"Unknown({prediction})")
        confidence = float(max(probabilities))
        
        print(f"üéØ Prediction Results:")
        print(f"   - Risk Level: {risk_level}")
        print(f"   - Confidence: {confidence:.1%}")
        print(f"   - Probabilities: Low={probabilities[0]:.3f}, Moderate={probabilities[1]:.3f}, High={probabilities[2]:.3f}")
        
        # Validate results
        if risk_level in ["Low", "Moderate", "High"] and 0 <= confidence <= 1:
            print("‚úÖ Risk model test passed")
            return True
        else:
            print("‚ùå Risk model test failed - invalid results")
            return False
            
    except Exception as e:
        print(f"‚ùå Risk model test failed: {str(e)}")
        return False

def test_goal_model():
    """Test the goal success prediction model with sample data."""
    print("\nüéØ Testing Goal Success Model")
    print("-" * 40)
    
    try:
        # Load models
        goal_model = joblib.load("models/goal_success_model.pkl")
        goal_encoders = joblib.load("models/goal_success_label_encoders.pkl")
        
        print("‚úÖ Goal models loaded")
        
        # Create sample data
        sample_data = {
            "age": 35,
            "annual_income": 800000,
            "current_savings": 500000,
            "target_goal_amount": 2000000,
            "investment_experience_level": "Intermediate",
            "investment_horizon_years": 10
        }
        
        print(f"üìù Testing goal: ‚Çπ{sample_data['target_goal_amount']:,} in {sample_data['investment_horizon_years']} years")
        
        # Prepare data
        input_df = pd.DataFrame([sample_data])
        
        # Encode categorical variables
        for col, encoder in goal_encoders.items():
            if col in input_df.columns:
                try:
                    original_value = input_df[col].iloc[0]
                    input_df[col] = encoder.transform(input_df[col])
                    print(f"   - Encoded {col}: '{original_value}' ‚Üí {input_df[col].iloc[0]}")
                except ValueError as e:
                    print(f"   ‚ö†Ô∏è Encoding issue for {col}: {e}")
                    # Use first class as fallback
                    input_df[col] = encoder.transform([encoder.classes_[0]])[0]
        
        # Make prediction
        if hasattr(goal_model, 'predict_proba'):
            # Classification model
            probabilities = goal_model.predict_proba(input_df)[0]
            prediction = goal_model.predict(input_df)[0]
            
            goal_success = "Likely" if prediction == 1 else "Unlikely"
            probability = float(probabilities[1]) if len(probabilities) > 1 else float(probabilities[0])
            
            print(f"üéØ Prediction Results (Classification):")
            print(f"   - Goal Success: {goal_success}")
            print(f"   - Probability: {probability:.1%}")
            if len(probabilities) > 1:
                print(f"   - Class Probabilities: {probabilities}")
        else:
            # Regression model
            probability = float(goal_model.predict(input_df)[0])
            goal_success = "Likely" if probability > 0.6 else "Unlikely"
            
            print(f"üéØ Prediction Results (Regression):")
            print(f"   - Goal Success: {goal_success}")
            print(f"   - Probability: {probability:.1%}")
        
        # Validate results
        if goal_success in ["Likely", "Unlikely"] and 0 <= probability <= 1:
            print("‚úÖ Goal model test passed")
            return True
        else:
            print("‚ùå Goal model test failed - invalid results")
            return False
            
    except Exception as e:
        print(f"‚ùå Goal model test failed: {str(e)}")
        return False

def test_agent_integration():
    """Test that agents can properly load and use the models."""
    print("\nü§ñ Testing Agent Integration")
    print("-" * 40)
    
    try:
        # Add current directory to path
        sys.path.insert(0, '.')
        
        # Test Risk Assessment Agent
        from langraph_agents.agents.risk_assessment_agent import RiskAssessmentAgent
        risk_agent = RiskAssessmentAgent()
        
        if risk_agent.risk_model is not None and risk_agent.label_encoders is not None:
            print("‚úÖ Risk Assessment Agent: Models loaded successfully")
            risk_integration = True
        else:
            print("‚ùå Risk Assessment Agent: Models not loaded")
            risk_integration = False
        
        # Test Goal Planning Agent
        from langraph_agents.agents.goal_planning_agent import GoalPlanningAgent
        goal_agent = GoalPlanningAgent()
        
        if goal_agent.goal_model is not None and goal_agent.goal_encoders is not None:
            print("‚úÖ Goal Planning Agent: Models loaded successfully")
            goal_integration = True
        else:
            print("‚ùå Goal Planning Agent: Models not loaded")
            goal_integration = False
        
        return risk_integration and goal_integration
        
    except Exception as e:
        print(f"‚ùå Agent integration test failed: {str(e)}")
        return False

def test_model_performance():
    """Test model performance with multiple samples."""
    print("\nüìä Testing Model Performance")
    print("-" * 40)
    
    try:
        # Load models
        risk_model = joblib.load("models/risk_profile_model.pkl")
        risk_encoders = joblib.load("models/label_encoders.pkl")
        goal_model = joblib.load("models/goal_success_model.pkl")
        goal_encoders = joblib.load("models/goal_success_label_encoders.pkl")
        
        # Test with multiple samples
        test_samples = [
            {"age": 25, "annual_income": 600000, "current_savings": 100000, "target_goal_amount": 1000000, 
             "investment_horizon_years": 15, "number_of_dependents": 0, "investment_experience_level": "Beginner"},
            {"age": 40, "annual_income": 1200000, "current_savings": 800000, "target_goal_amount": 3000000, 
             "investment_horizon_years": 8, "number_of_dependents": 2, "investment_experience_level": "Advanced"},
            {"age": 55, "annual_income": 800000, "current_savings": 1500000, "target_goal_amount": 2000000, 
             "investment_horizon_years": 5, "number_of_dependents": 1, "investment_experience_level": "Intermediate"}
        ]
        
        print(f"Testing with {len(test_samples)} sample prospects...")
        
        risk_predictions = []
        goal_predictions = []
        
        for i, sample in enumerate(test_samples, 1):
            print(f"\nüìù Sample {i}: Age {sample['age']}, Income ‚Çπ{sample['annual_income']:,}")
            
            # Risk prediction
            risk_df = pd.DataFrame([sample])
            for col, encoder in risk_encoders.items():
                if col in risk_df.columns:
                    try:
                        risk_df[col] = encoder.transform(risk_df[col])
                    except ValueError:
                        risk_df[col] = encoder.transform([encoder.classes_[0]])[0]
            
            risk_pred = risk_model.predict(risk_df)[0]
            risk_proba = risk_model.predict_proba(risk_df)[0]
            risk_level = {0: "Low", 1: "Moderate", 2: "High"}[risk_pred]
            risk_predictions.append(risk_level)
            
            # Goal prediction
            goal_df = pd.DataFrame([{k: v for k, v in sample.items() if k != 'number_of_dependents'}])
            for col, encoder in goal_encoders.items():
                if col in goal_df.columns:
                    try:
                        goal_df[col] = encoder.transform(goal_df[col])
                    except ValueError:
                        goal_df[col] = encoder.transform([encoder.classes_[0]])[0]
            
            if hasattr(goal_model, 'predict_proba'):
                goal_proba = goal_model.predict_proba(goal_df)[0]
                goal_prob = float(goal_proba[1]) if len(goal_proba) > 1 else float(goal_proba[0])
            else:
                goal_prob = float(goal_model.predict(goal_df)[0])
            
            goal_success = "Likely" if goal_prob > 0.6 else "Unlikely"
            goal_predictions.append(goal_success)
            
            print(f"   - Risk: {risk_level} ({max(risk_proba):.1%} confidence)")
            print(f"   - Goal: {goal_success} ({goal_prob:.1%} probability)")
        
        # Summary
        print(f"\nüìä Performance Summary:")
        print(f"   - Risk Levels: {dict(pd.Series(risk_predictions).value_counts())}")
        print(f"   - Goal Success: {dict(pd.Series(goal_predictions).value_counts())}")
        print("‚úÖ Model performance test completed")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Model performance test failed: {str(e)}")
        return False

def generate_model_report():
    """Generate a comprehensive model report."""
    print("\nüìã Generating Model Report")
    print("-" * 40)
    
    try:
        # Load all models
        risk_model = joblib.load("models/risk_profile_model.pkl")
        risk_encoders = joblib.load("models/label_encoders.pkl")
        goal_model = joblib.load("models/goal_success_model.pkl")
        goal_encoders = joblib.load("models/goal_success_label_encoders.pkl")
        
        report = []
        report.append("# ML Models Report")
        report.append("=" * 50)
        report.append("")
        
        # Risk Model Info
        report.append("## Risk Assessment Model")
        report.append(f"- Model Type: {type(risk_model).__name__}")
        if hasattr(risk_model, 'classes_'):
            report.append(f"- Classes: {list(risk_model.classes_)}")
        if hasattr(risk_model, 'feature_names_in_'):
            report.append(f"- Features: {list(risk_model.feature_names_in_)}")
        report.append(f"- Risk Encoders: {list(risk_encoders.keys())}")
        report.append("")
        
        # Goal Model Info
        report.append("## Goal Success Model")
        report.append(f"- Model Type: {type(goal_model).__name__}")
        if hasattr(goal_model, 'classes_'):
            report.append(f"- Classes: {list(goal_model.classes_)}")
        if hasattr(goal_model, 'feature_names_in_'):
            report.append(f"- Features: {list(goal_model.feature_names_in_)}")
        report.append(f"- Goal Encoders: {list(goal_encoders.keys())}")
        report.append("")
        
        # Save report
        with open("models/MODEL_REPORT.md", "w") as f:
            f.write("\n".join(report))
        
        print("‚úÖ Model report generated: models/MODEL_REPORT.md")
        return True
        
    except Exception as e:
        print(f"‚ùå Failed to generate model report: {str(e)}")
        return False

def main():
    """Run comprehensive model testing."""
    print("üß™ ML Models Validation Suite")
    print("=" * 50)
    
    tests = [
        ("Model Files", test_model_files),
        ("Risk Model", test_risk_model),
        ("Goal Model", test_goal_model),
        ("Agent Integration", test_agent_integration),
        ("Model Performance", test_model_performance),
        ("Model Report", generate_model_report),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå {test_name} test crashed: {e}")
            results.append((test_name, False))
    
    # Summary
    print("\n" + "=" * 50)
    print("üìä MODEL VALIDATION SUMMARY")
    print("=" * 50)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status}: {test_name}")
    
    print(f"\nOverall: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nüéâ All model tests passed! Your ML models are ready for use.")
        print("The system will now use ML-based predictions with high accuracy.")
    else:
        print(f"\n‚ö†Ô∏è {total - passed} tests failed. Some models may not work correctly.")
        print("The system will fall back to rule-based predictions where needed.")
    
    print("\nüìã Next Steps:")
    print("1. Check models/MODEL_REPORT.md for detailed model information")
    print("2. Run: streamlit run main.py")
    print("3. Test with real prospects to see ML predictions in action")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

import asyncio
import sys
import pytest
from datetime import datetime
from typing import Dict, Any

def test_imports():
    """Test all critical imports."""
    print("üîç Testing imports...")
    
    try:
        import streamlit as st
        print("‚úÖ Streamlit imported successfully")
    except ImportError as e:
        print(f"‚ùå Streamlit import failed: {e}")
        return False
    
    try:
        from graph import ProspectAnalysisWorkflow
        print("‚úÖ LangGraph workflow imported successfully")
    except ImportError as e:
        print(f"‚ùå LangGraph workflow import failed: {e}")
        return False
    
    try:
        from config.settings import get_settings
        print("‚úÖ Settings imported successfully")
    except ImportError as e:
        print(f"‚ùå Settings import failed: {e}")
        return False
    
    try:
        from langraph_agents.agents.data_analyst_agent import DataAnalystAgent
        from langraph_agents.agents.risk_assessment_agent import RiskAssessmentAgent
        from langraph_agents.agents.persona_agent import PersonaAgent
        from langraph_agents.agents.product_specialist_agent import ProductSpecialistAgent
        print("‚úÖ All agents imported successfully")
    except ImportError as e:
        print(f"‚ùå Agent import failed: {e}")
        return False
    
    return True

def test_configuration():
    """Test configuration loading."""
    print("\nüîß Testing configuration...")
    
    try:
        from config.settings import get_settings
        settings = get_settings()
        
        if settings.gemini_api_key and len(settings.gemini_api_key) > 10:
            print("‚úÖ Gemini API key configured")
        else:
            print("‚ö†Ô∏è  Gemini API key not configured or too short")
            return False
        
        print(f"‚úÖ Log level: {settings.log_level}")
        print(f"‚úÖ Max concurrent agents: {settings.max_concurrent_agents}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Configuration test failed: {e}")
        return False

def test_data_loading():
    """Test data file loading."""
    print("\nüìä Testing data loading...")
    
    try:
        import pandas as pd
        from config.settings import get_settings
        settings = get_settings()
        
        # Test prospects data
        try:
            prospects_df = pd.read_csv(settings.prospects_csv)
            print(f"‚úÖ Prospects data loaded: {len(prospects_df)} records")
        except Exception as e:
            print(f"‚ö†Ô∏è  Prospects data loading failed: {e}")
        
        # Test products data
        try:
            products_df = pd.read_csv(settings.products_csv)
            print(f"‚úÖ Products data loaded: {len(products_df)} records")
        except Exception as e:
            print(f"‚ö†Ô∏è  Products data loading failed: {e}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Data loading test failed: {e}")
        return False

def test_agent_initialization():
    """Test agent initialization."""
    print("\nü§ñ Testing agent initialization...")
    
    try:
        from langraph_agents.agents.data_analyst_agent import DataAnalystAgent
        from langraph_agents.agents.risk_assessment_agent import RiskAssessmentAgent
        from langraph_agents.agents.persona_agent import PersonaAgent
        from langraph_agents.agents.product_specialist_agent import ProductSpecialistAgent
        
        # Initialize agents
        data_analyst = DataAnalystAgent()
        print(f"‚úÖ {data_analyst.name} initialized")
        
        risk_assessor = RiskAssessmentAgent()
        print(f"‚úÖ {risk_assessor.name} initialized")
        
        persona_classifier = PersonaAgent()
        print(f"‚úÖ {persona_classifier.name} initialized")
        
        product_specialist = ProductSpecialistAgent()
        print(f"‚úÖ {product_specialist.name} initialized")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Agent initialization failed: {e}")
        return False

def test_workflow_creation():
    """Test workflow creation."""
    print("\nüîÑ Testing workflow creation...")
    
    try:
        from graph import ProspectAnalysisWorkflow
        
        workflow = ProspectAnalysisWorkflow()
        print("‚úÖ Workflow created successfully")
        
        summary = workflow.get_workflow_summary()
        print(f"‚úÖ Workflow has {len(summary['agents'])} agents")
        print(f"‚úÖ Workflow has {len(summary['steps'])} steps")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Workflow creation failed: {e}")
        return False

@pytest.mark.asyncio
async def test_sample_analysis():
    """Test sample prospect analysis."""
    print("\nüß™ Testing sample analysis...")

    try:
        from graph import ProspectAnalysisWorkflow

        # Sample prospect data
        sample_prospect = {
            "prospect_id": "TEST001",
            "name": "Test Client",
            "age": 35,
            "annual_income": 800000,
            "current_savings": 500000,
            "target_goal_amount": 2000000,
            "investment_horizon_years": 10,
            "number_of_dependents": 2,
            "investment_experience_level": "Intermediate",
            "investment_goal": "Test Goal"
        }

        workflow = ProspectAnalysisWorkflow()
        print("‚úÖ Starting sample analysis...")

        # Run analysis with timeout
        try:
            result = await asyncio.wait_for(
                workflow.analyze_prospect(sample_prospect),
                timeout=120  # 2 minute timeout
            )

            print("‚úÖ Sample analysis completed successfully")

            # Check results
            if result.analysis.risk_assessment:
                print(f"‚úÖ Risk assessment: {result.analysis.risk_assessment.risk_level}")

            if result.analysis.persona_classification:
                print(f"‚úÖ Persona: {result.analysis.persona_classification.persona_type}")

            if result.recommendations.recommended_products:
                print(f"‚úÖ Recommendations: {len(result.recommendations.recommended_products)} products")

            exec_summary = result.get_execution_summary()
            print(f"‚úÖ Execution summary: {exec_summary['success_rate']:.1%} success rate")

            return True

        except asyncio.TimeoutError:
            print("‚ö†Ô∏è  Sample analysis timed out (this may be due to API rate limits)")
            return True  # Don't fail the test for timeout

    except Exception as e:
        print(f"‚ùå Sample analysis failed: {e}")
        return False

def test_logging():
    """Test logging configuration."""
    print("\nüìù Testing logging...")
    
    try:
        from config.logging_config import setup_logging, get_logger
        
        setup_logging()
        logger = get_logger("TestLogger")
        
        logger.info("Test log message")
        print("‚úÖ Logging configured successfully")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Logging test failed: {e}")
        return False

async def run_all_tests():
    """Run all system tests."""
    print("üöÄ Starting RM-AgenticAI-LangGraph System Tests")
    print("=" * 60)
    
    tests = [
        ("Import Test", test_imports),
        ("Configuration Test", test_configuration),
        ("Data Loading Test", test_data_loading),
        ("Agent Initialization Test", test_agent_initialization),
        ("Workflow Creation Test", test_workflow_creation),
        ("Logging Test", test_logging),
        ("Sample Analysis Test", test_sample_analysis)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        
        try:
            if asyncio.iscoroutinefunction(test_func):
                result = await test_func()
            else:
                result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå {test_name} crashed: {e}")
            results.append((test_name, False))
    
    # Summary
    print("\n" + "="*60)
    print("üìä TEST SUMMARY")
    print("="*60)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"{test_name:<30} {status}")
        if result:
            passed += 1
    
    print(f"\nOverall Result: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("üéâ All tests passed! System is ready to use.")
        return True
    else:
        print("‚ö†Ô∏è  Some tests failed. Please check the configuration and try again.")
        return False

def main():
    """Main test function."""
    try:
        # Run async tests
        result = asyncio.run(run_all_tests())
        sys.exit(0 if result else 1)
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Tests interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Test suite crashed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()